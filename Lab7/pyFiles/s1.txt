1. Содержательный подход
Для равновероятных событий используется формула Хартли I = log2N, где N – количество
возможных равновероятных событий, I – количество информации.
Пример 1. В барабане для розыгрыша лотереи находится 32 шара. Сколько информации
содержит сообщение о первом выпавшем номере (например, выпал номер 15)?
Решение:
т.к. вытаскивание любого из шаров равновероятно, то количество информации вычисляется по
формуле 2I=N, где I – количество информации, а N – количество шаров. Тогда 2I=32, отсюда I = 5
бит.
Ответ: 5 бит.
Пример 2. Какой объем информации содержит сообщение, уменьшающее неопределенность
знаний в 4 раза?
Решение:
1 бит – это количество информации, уменьшающее неопределенность знаний в 2 раза.
I бит – это количество информации, уменьшающее неопределенность знаний в 2I
раз, отсюда,
4=2I
, I = 2 бит.
Ответ: 2 бит.
Пример 3. При угадывании целого числа в некотором диапазоне было получено 8 бит
информации. Сколько чисел содержал этот диапазон?
Решение:
i=8 бит
N=28
N=256
Ответ: 256 чисел в диапазоне.
Для реализации Примера 4 – Примера 5 необходимо использовать Калькулятор Windows.
Пример 4.
Какое количество информации несет сообщение: «Встреча назначена на сентябрь».
Решение:
Поскольку появление в сообщении месяца сентябрь равновероятно из 12 месяцев, то количество
информации определяется по формуле: 2I=N, где I – количество информации, N – количество
месяцев. Отсюда: 2I=12, I=log212≈3.584962501 бит.
Пример 5.
Какое количество информации несет сообщение о том, что встреча назначена на 23 октября в
15.00?
Решение:
Поскольку появление в сообщении определенного числа месяца, определенного месяца и
определенного часа равновероятно из общего числа дней в месяце, общего числа месяцев, общего
числа часов, то количество информации определяется по формуле: 2I=N, где I – количество
информации, N=31*12*24 – (количество дней в месяце)*(количество месяцев)*(количество часов в
сутках). Отсюда: 2I=31*12*24, I=log2(31*12*24)≈13.12412131 бит.
2. Вероятностный подход
Формула Шеннона
Так как наступление каждого из N событий имеет одинаковую вероятность P, то Р=1/N. Если
событий 6, то вероятность появления одного события равно 1/6, если событий 100, то вероятность
равна 0,01=>N=1/P.
Американский ученый Клод Шеннон, что формулу Хартли можно записать иначе: I=log2(1/P) =
log2P
-1= - log2P, так как p <1, то log2P <0, а I>0 (минус на минус дает плюс)
В 1948 год он предложил другую формулу определения количества информации, учитывая
возможную неодинаковую вероятность событий в наборе.
Для событий с различными вероятностями количество информации определяется по формуле
Шеннона:
i
N
i
I Pi 2 P
1
 log

  , где I – количество информации, N – количество возможных не
равновероятных событий, Pi– вероятности отдельных событий.
При решении задач необходимо подчеркнуть, что при равновероятных событиях мы получаем
большее количество информации, чем при неравно вероятных событиях.
Пример 6.
В корзине лежат 8 черных шаров и 24 белых. Сколько информации несет сообщение о том, что
достали черный шар?